{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementations follows the lecture by Andrej Karpathy.\n",
    "https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "\n",
    "Concepts are delved into following 3blue1browns series on neural networks\n",
    "https://www.youtube.com/watch?v=aircAruvnKk\n",
    "\n",
    "\n",
    "TODO work on implementing OpenWebText api as the input data. This is 38GB worth of text input data. May need some preprocessing like removing special characters, and lower casing everything.\n",
    "\n",
    "dataset = load_dataset(\"Skylion007/openwebtext\")\n",
    "\n",
    "https://paperswithcode.com/dataset/openwebtext\n",
    "\n",
    "TODO Try using a better tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset: 1115394\n",
      "length of vocabulary: 65\n",
      "vocabulary: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# with open(\"alice.txt\", encoding=\"utf-8\") as f:\n",
    "#     text = f.read()\n",
    "\n",
    "with open(\"shakespeare.txt\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset:\", len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars) # note that capital and small letters are treated as different characters\n",
    "print(\"length of vocabulary:\", vocab_size)\n",
    "print(\"vocabulary:\", ''.join(chars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build encoder and decoder\n",
    "The encoders job is translating the vocabulary into integers\n",
    "The decoders job is to reverse this encoding turning it back into the original character\n",
    "\n",
    "Encoders can follow different schemas, popular implementations are tiktoken (chatGPT) and sentencepiece (Google). The encoders are sub word encoders, meaning that they don't follow a simple schema of just converting each unique word into a token. This means words can be broken into tokens partly into the word. This leads to a lot more tokens being generated, which means a sentence can be broken down into a short sequence of integers.\n",
    "\n",
    "For intuition this implementation of encoding and decoding will use a simple encoder, which encodes per character, meaning it will generate a long sequence of small tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without encoder function [46, 43, 50, 50, 53]\n",
      "With encoder function [46, 43, 50, 50, 53]\n",
      "Decoded:  hello\n"
     ]
    }
   ],
   "source": [
    "# create dictionaries to convert characters to integers and vice versa\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "\n",
    "# encode the text\n",
    "# lambda functions are used as small throwaway functions\n",
    "encoder = lambda string: [char_to_int[char] for char in string] # make a list of every encoded character in input string\n",
    "decoder = lambda string: ''.join([int_to_char[i] for i in string]) # reverse the encoding\n",
    "\n",
    "print(\"Without encoder function\", [char_to_int[\"h\"], char_to_int[\"e\"], char_to_int[\"l\"], char_to_int[\"l\"], char_to_int[\"o\"]])\n",
    "print(\"With encoder function\", encoder(\"hello\"))\n",
    "\n",
    "print(\"Decoded: \", decoder(encoder(\"hello\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset\n",
    "This section encodes the entire dataset and splits the data into a train portion and a validation portion.\n",
    "The data will be stored in a tensor object from PyTorch.\n",
    "\n",
    "Data loaders will be made as the transformer will need batches of data to train on instead of feeding it the entire dataset in one go. Remember that when the batch is fed to the transformer, it will try to get a prediction for each example in the batch. This example will be dependent on the context of the words before it, but shouldn't be influenced by the words after it. This means that the target, x, should be influenced by the context, [0:x-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "when context is tensor([18]) target is 47\n",
      "when context is tensor([18, 47]) target is 56\n",
      "when context is tensor([18, 47, 56]) target is 57\n",
      "when context is tensor([18, 47, 56, 57]) target is 58\n",
      "when context is tensor([18, 47, 56, 57, 58]) target is 1\n",
      "tensor(43)\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encoder(text), dtype=torch.long) # this is a 1D vector with an integer for each character in the entire text\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "#reserve 10% of the data for validation\n",
    "train_size = int(len(data) * 0.9)\n",
    "train_data = data[0:train_size]\n",
    "val_data = data[train_size:len(data)]\n",
    "\n",
    "\n",
    "# explaination function DON'T USE\n",
    "def data_loader_explaination(data, block_size):\n",
    "    # block_size decides the amount of context that should be included in training\n",
    "    batch = data[:block_size + 1]\n",
    "    x = batch[:block_size]\n",
    "    y = batch[1:block_size + 1] # y is the same as x, but shifted by one character\n",
    "    \n",
    "    for i in range(block_size):\n",
    "        context = x[:i+1]\n",
    "        target = y[i]\n",
    "        print(f\"when context is {context} target is {target}\")\n",
    "    \n",
    "\n",
    "data_loader_explaination(train_data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loader\n",
    "A batch is defined as how many \"trainings\" should run parallel. These trainings will have nothing to do with each other, but are purely for optimization.\n",
    "Block_size is defined as the amount of words to include in a single training. The context. Remember that the block will contain size-1 elements.\n",
    "\n",
    "Block_size is also refered to as time, T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  torch.Size([4, 8]) \n",
      " tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]]) \n",
      "\n",
      " Outputs:  torch.Size([4, 8]) \n",
      " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "seed = 1337\n",
    "torch.manual_seed(seed) # seeded randomness\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(mode):\n",
    "    if mode == \"train\":\n",
    "        data = train_data\n",
    "    elif mode == \"val\":\n",
    "        data = val_data\n",
    "    start_idx = torch.randint(0, len(data) - block_size, (batch_size,)) # get batch_size number of randoms between 0 and (length of data - block_size)\n",
    "    \n",
    "    \n",
    "    # these loops pick a start index from start_ids and store that + block_size characters in context and targets\n",
    "    # targets is offset by one character from context\n",
    "    context = torch.stack([data[i:i+block_size] for i in start_idx]) # shape: (batch_size, block_size)\n",
    "    targets = torch.stack([data[i+1:i+1+block_size] for i in start_idx]) # shape: (batch_size, block_size)\n",
    "    return context, targets\n",
    "\n",
    "context, targets = get_batch(\"train\")\n",
    "print(\"inputs: \", context.shape, \"\\n\", context, \"\\n\\n Outputs: \", targets.shape, \"\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a neural network with pytorch\n",
    "A bigram model is used here. A bigram model only considers the previous token, which means everything about context is wasted for now, but still implemented\n",
    "\n",
    "TODO read up on this in video from Andrej \n",
    "\n",
    "Tokens are embedded into a lookup table using nn.Embedding\n",
    "\n",
    "One-hot encoding is encoding values into categorical number tables\n",
    "\n",
    "![alt text](https://miro.medium.com/v2/resize:fit:1400/1*ggtP4a5YaRx6l09KQaYOnw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([32, 65]) \n",
      " loss:  tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed) # seeded randomness\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size) # a lookup table where rows are plucked out based on the input token (one-hot encoded)\n",
    "        \n",
    "    def forward(self, idx, targets = None):\n",
    "        logits = self.embedding_table(idx) # shape: (batch_size, block_size, vocab_size) OR (B, T, C)\n",
    "        \n",
    "        # failsafe if true identity of next token is not known\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # targets contain the identity of the next character, cross_entropy computes the quality of the prediction in logits\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # value up, B, and value down, T, matrices from 3blue1brown \n",
    "            targets = targets.view(B*T)\n",
    "            \n",
    "            loss = F.cross_entropy(logits, targets) \n",
    "        \n",
    "        # logits are scores for each token use to predict the next token e.g. certain characters are more likely to follow others\n",
    "        return logits, loss\n",
    "        \n",
    "    def predict_next(self, idx, max_new_tokens):\n",
    "        # idx is the context\n",
    "        for i in range(max_new_tokens):\n",
    "            # get predictions (logit is the output before applying an activation function)\n",
    "            logits, loss = self.forward(idx) # currently feeding in the entire context, but only need the last token\n",
    "            # store only the last prediction\n",
    "            logits = logits[:,-1, :]\n",
    "            # convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # pick sample\n",
    "            next_token = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append predicted token to context\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "        return idx\n",
    "   \n",
    "\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model.forward(context, targets)\n",
    "print(\"shape: \", logits.shape, \"\\n loss: \", loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decoder(model.predict_next(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "AdamW is used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4212486743927\n"
     ]
    }
   ],
   "source": [
    "def train_model(iterations):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    for step in range(iterations):\n",
    "        x_batch, y_batch = get_batch(\"train\")\n",
    "        logits, loss = model(x_batch, y_batch)\n",
    "        optimizer.zero_grad(set_to_none=True) # set_to_none is a memory optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss.item())\n",
    "train_model(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WIs stllomu?'s.\n",
      "I pr cavo.\n",
      "iprclinn sd su gnce!\n",
      "bes, athour fodehiou, f yee be bonn?RKERUvinod mumanst!\n",
      "Thisltit po at: mm; do a Is?nste:\n",
      "Awnepthedethans fo tatexven;I dg avatofal msur d'se-WChes cre ward y 'TzWA ss l m?p a atmy biker: ttheamepliveromo;\n",
      "Thin\n",
      "Slswirise.\n",
      "DURTh iminifultsene'shriss chal!uim; le fisthFLonehe\n",
      "MOKHY d ar,\n",
      "LAM:\n",
      "GRinasi'd nocou indo'ASI tst o h tu ckxBO:\n",
      "\n",
      "Torfat tyemy d-$gnshiBis cof yss O:\n",
      "Bupoins!\n",
      "Jd lathed:\n",
      "\n",
      "\n",
      "Ger jur\n",
      "Es araty,\n",
      "I sper tornd as ho t h tin, t masu kivin\n"
     ]
    }
   ],
   "source": [
    "print(decoder(model.predict_next(idx, max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
