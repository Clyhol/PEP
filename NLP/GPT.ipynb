{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementations follows the lecture by Andrej Karpathy.\n",
    "https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "\n",
    "Concepts are delved into following 3blue1browns series on neural networks\n",
    "https://www.youtube.com/watch?v=aircAruvnKk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset: 163434\n",
      "length of vocabulary: 91\n",
      "vocabulary: \n",
      " !#$%'()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyzù—‘’“”•™﻿\n"
     ]
    }
   ],
   "source": [
    "with open(\"alice.txt\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset:\", len(text))\n",
    "\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars) # note that capital and small letters are treated as different characters\n",
    "print(\"length of vocabulary:\", vocab_size)\n",
    "print(\"vocabulary:\", ''.join(chars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build encoder and decoder\n",
    "The encoders job is translating the vocabulary into integers\n",
    "The decoders job is to reverse this encoding turning it back into the original character\n",
    "\n",
    "Encoders can follow different schemas, popular implementations are tiktoken (chatGPT) and sentencepiece (Google). The encoders are sub word encoders, meaning that they don't follow a simple schema of just converting each unique word into a token. This means words can be broken into tokens partly into the word. This leads to a lot more tokens being generated, which means a sentence can be broken down into a short sequence of integers.\n",
    "\n",
    "For intuition this implementation of encoding and decoding will use a simple encoder, which encodes per character, meaning it will generate a long sequence of small tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without encoder function [63, 60, 67, 67, 70]\n",
      "With encoder function [63, 60, 67, 67, 70]\n",
      "Decoded ['h', 'e', 'l', 'l', 'o']\n"
     ]
    }
   ],
   "source": [
    "# create dictionaries to convert characters to integers and vice versa\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "\n",
    "# encode the text\n",
    "# lambda functions are used as small throwaway functions\n",
    "encoder = lambda string: [char_to_int[char] for char in string] # make a list of every encoded character in input string\n",
    "decoder = lambda string: [int_to_char[i] for i in string] # reverse the encoding\n",
    "\n",
    "print(\"Without encoder function\", [char_to_int[\"h\"], char_to_int[\"e\"], char_to_int[\"l\"], char_to_int[\"l\"], char_to_int[\"o\"]])\n",
    "print(\"With encoder function\", encoder(\"hello\"))\n",
    "\n",
    "print(\"Decoded\", decoder(encoder(\"hello\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset\n",
    "This section encodes the entire dataset and splits the data into a train portion and a validation portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded data [90, 46, 64, 75, 67, 60, 24, 1, 27, 67]\n",
      "Decoded data ['\\ufeff', 'T', 'i', 't', 'l', 'e', ':', ' ', 'A', 'l']\n"
     ]
    }
   ],
   "source": [
    "data = encoder(text)\n",
    "print(\"Encoded data\", data[:10])\n",
    "print (\"Decoded data\", decoder(data[:10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
