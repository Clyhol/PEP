{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementations follows the lecture by Andrej Karpathy.\n",
    "https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "\n",
    "Concepts are delved into following 3blue1browns series on neural networks\n",
    "https://www.youtube.com/watch?v=aircAruvnKk\n",
    "\n",
    "\n",
    "TODO work on implementing OpenWebText api as the input data. This is 38GB worth of text input data. May need some preprocessing like removing special characters, and lower casing everything.\n",
    "\n",
    "dataset = load_dataset(\"Skylion007/openwebtext\")\n",
    "\n",
    "https://paperswithcode.com/dataset/openwebtext\n",
    "\n",
    "TODO Try using a better tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset: 1115394\n",
      "length of vocabulary: 65\n",
      "vocabulary: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# with open(\"alice.txt\", encoding=\"utf-8\") as f:\n",
    "#     text = f.read()\n",
    "\n",
    "with open(\"shakespeare.txt\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset:\", len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars) # note that capital and small letters are treated as different characters\n",
    "print(\"length of vocabulary:\", vocab_size)\n",
    "print(\"vocabulary:\", ''.join(chars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build encoder and decoder\n",
    "The encoders job is translating the vocabulary into integers\n",
    "The decoders job is to reverse this encoding turning it back into the original character\n",
    "\n",
    "Encoders can follow different schemas, popular implementations are tiktoken (chatGPT) and sentencepiece (Google). The encoders are sub word encoders, meaning that they don't follow a simple schema of just converting each unique word into a token. This means words can be broken into tokens partly into the word. This leads to a lot more tokens being generated, which means a sentence can be broken down into a short sequence of integers.\n",
    "\n",
    "For intuition this implementation of encoding and decoding will use a simple encoder, which encodes per character, meaning it will generate a long sequence of small tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without encoder function [46, 43, 50, 50, 53]\n",
      "With encoder function [46, 43, 50, 50, 53]\n",
      "Decoded:  hello\n"
     ]
    }
   ],
   "source": [
    "# create dictionaries to convert characters to integers and vice versa\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "\n",
    "# encode the text\n",
    "# lambda functions are used as small throwaway functions\n",
    "encoder = lambda string: [char_to_int[char] for char in string] # make a list of every encoded character in input string\n",
    "decoder = lambda string: ''.join([int_to_char[i] for i in string]) # reverse the encoding\n",
    "\n",
    "print(\"Without encoder function\", [char_to_int[\"h\"], char_to_int[\"e\"], char_to_int[\"l\"], char_to_int[\"l\"], char_to_int[\"o\"]])\n",
    "print(\"With encoder function\", encoder(\"hello\"))\n",
    "\n",
    "print(\"Decoded: \", decoder(encoder(\"hello\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset\n",
    "This section encodes the entire dataset and splits the data into a train portion and a validation portion.\n",
    "The data will be stored in a tensor object from PyTorch.\n",
    "\n",
    "Data loaders will be made as the transformer will need batches of data to train on instead of feeding it the entire dataset in one go. Remember that when the batch is fed to the transformer, it will try to get a prediction for each example in the batch. This example will be dependent on the context of the words before it, but shouldn't be influenced by the words after it. This means that the target, x, should be influenced by the context, [0:x-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "when context is tensor([18]) target is 47\n",
      "when context is tensor([18, 47]) target is 56\n",
      "when context is tensor([18, 47, 56]) target is 57\n",
      "when context is tensor([18, 47, 56, 57]) target is 58\n",
      "when context is tensor([18, 47, 56, 57, 58]) target is 1\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encoder(text), dtype=torch.long) # this is a 1D vector with an integer for each character in the entire text\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "#reserve 10% of the data for validation\n",
    "train_size = int(len(data) * 0.9)\n",
    "train_data = data[0:train_size]\n",
    "val_data = data[train_size:len(data)]\n",
    "\n",
    "\n",
    "# explaination function DON'T USE\n",
    "def data_loader_explaination(data, block_size):\n",
    "    # block_size decides the amount of context that should be included in training\n",
    "    batch = data[:block_size + 1]\n",
    "    x = batch[:block_size]\n",
    "    y = batch[1:block_size + 1] # y is the same as x, but shifted by one character\n",
    "    \n",
    "    for i in range(block_size):\n",
    "        context = x[:i+1]\n",
    "        target = y[i]\n",
    "        print(f\"when context is {context} target is {target}\")\n",
    "    \n",
    "\n",
    "data_loader_explaination(train_data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loader\n",
    "A batch is defined as how many \"trainings\" should run parallel. These trainings will have nothing to do with each other, but are purely for optimization.\n",
    "Block_size is defined as the amount of words to include in a single training. The context. Remember that the block will contain size-1 elements.\n",
    "\n",
    "Block_size is also refered to as time, T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  torch.Size([4, 8]) \n",
      " tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]]) \n",
      "\n",
      " Outputs:  torch.Size([4, 8]) \n",
      " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "seed = 1337\n",
    "torch.manual_seed(seed) # seeded randomness\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(mode):\n",
    "    if mode == \"train\":\n",
    "        data = train_data\n",
    "    elif mode == \"val\":\n",
    "        data = val_data\n",
    "    start_idx = torch.randint(0, len(data) - block_size, (batch_size,)) # get batch_size number of randoms between 0 and (length of data - block_size)\n",
    "    \n",
    "    \n",
    "    # these loops pick a start index from start_ids and store that + block_size characters in context and targets\n",
    "    # targets is offset by one character from context\n",
    "    context = torch.stack([data[i:i+block_size] for i in start_idx]) # shape: (batch_size, block_size)\n",
    "    targets = torch.stack([data[i+1:i+1+block_size] for i in start_idx]) # shape: (batch_size, block_size)\n",
    "    return context, targets\n",
    "\n",
    "context, targets = get_batch(\"train\")\n",
    "print(\"inputs: \", context.shape, \"\\n\", context, \"\\n\\n Outputs: \", targets.shape, \"\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a neural network with pytorch\n",
    "A bigram model is used here. A bigram model only considers the previous token, which means everything about context is wasted for now, but still implemented\n",
    "\n",
    "TODO read up on this in video from Andrej \n",
    "\n",
    "Tokens are embedded into a lookup table using nn.Embedding\n",
    "\n",
    "One-hot encoding is encoding values into categorical number tables\n",
    "\n",
    "![alt text](https://miro.medium.com/v2/resize:fit:1400/1*ggtP4a5YaRx6l09KQaYOnw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([32, 65]) \n",
      " loss:  tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed) # seeded randomness\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size) # a lookup table where rows are plucked out based on the input token (one-hot encoded)\n",
    "        \n",
    "    def forward(self, idx, targets = None):\n",
    "        logits = self.embedding_table(idx) # shape: (batch_size, block_size, vocab_size) OR (B, T, C)\n",
    "        \n",
    "        # failsafe if true identity of next token is not known\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # targets contain the identity of the next character, cross_entropy computes the quality of the prediction in logits\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # value up, B, and value down, T, matrices from 3blue1brown \n",
    "            targets = targets.view(B*T)\n",
    "            \n",
    "            loss = F.cross_entropy(logits, targets) \n",
    "        \n",
    "        # logits are scores for each token use to predict the next token e.g. certain characters are more likely to follow others\n",
    "        return logits, loss\n",
    "        \n",
    "    def predict_next(self, idx, max_new_tokens):\n",
    "        # idx is the context\n",
    "        for i in range(max_new_tokens):\n",
    "            # get predictions (logit is the output before applying an activation function)\n",
    "            logits, loss = self.forward(idx) # currently feeding in the entire context, but only need the last token\n",
    "            # store only the last prediction\n",
    "            logits = logits[:,-1, :]\n",
    "            # convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # pick sample\n",
    "            next_token = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append predicted token to context\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "        return idx\n",
    "   \n",
    "\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model.forward(context, targets)\n",
    "print(\"shape: \", logits.shape, \"\\n loss: \", loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decoder(model.predict_next(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "AdamW is used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6538658142089844\n"
     ]
    }
   ],
   "source": [
    "def train_model(iterations):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    for step in range(iterations):\n",
    "        x_batch, y_batch = get_batch(\"train\")\n",
    "        logits, loss = model(x_batch, y_batch)\n",
    "        optimizer.zero_grad(set_to_none=True) # set_to_none is a memory optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss.item())\n",
    "train_model(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Th hie yoralk pory Gou, Indepal winferoisthanard heas ar hentho o mias g utthefldeascigonfe\n",
      "\n",
      "RO lt thend ye, we fr ho rgoof stims Isllou andid fis fer de\n",
      "OR:\n",
      "An, thas phalil ll! bat! whorar d y\n",
      "\n",
      "Wam; your foulen thit cope f, tot\n",
      "W: brcaire thalesthe astowowiletift te r h w, ho s dure usierd l thent fo tememiout, m w on prcut HORI imy\n",
      "KIke ats he urot h.\n",
      "NG a br.\n",
      "WAS:\n",
      "\n",
      "A busat he totity gice u s:\n",
      "Wil, he. ar t Gouckeve, magmbik bre IOVHes the lit s, Key hes indeod!\n",
      "EG imounonthe dr.\n",
      "whtly there k f cer.\n",
      "My twof ilen rtoust pall weedy yous hee ke t mir f y P, fe hervathorn t?\n",
      "GNTUS y r Caide tamacoubutigathivear,\n",
      "ORAnowh y, of bs ourererechathand gordsen ofarske ththethoft s; home nicye bt pr,\n",
      "Bifras thend'ert,\n",
      "\n",
      "Fortre ssh'ayor win deaushs:\n",
      "Where onoun htunonorerind ltrerinitshad mo for I iblvis?\n",
      "R:\n",
      "\n",
      "I end mere, wenthanod ara atay weree YWhifongne m toust wooboas m y, cLINGShe steye l! N:\n",
      "Fl d INERrmy ofelos deily tould. st mouegr allour.\n",
      "ED:\n",
      "JK: go zese, itee t arue\n",
      "\n",
      "OLI suldeve harkerer?\n",
      "\n",
      "\n",
      "Whisow?\n",
      "Vgor. wdichir the is m br fo p,\n",
      "wh Boacelserice.\n",
      "OLOMQUSins ine th sofeofoopill tet emise s, fithar me Jubeer s oe collesforefonownd nt t f, meger; ts, hourie se chinngis: r ceshif\n",
      "\n",
      "And\n",
      "\n",
      "\n",
      "\n",
      "QUKil, r lunckirlous gad the alors dvenove,\n",
      "The lont'd chand'st ar nayof bors w's helffie.\n",
      "Anss S:\n",
      "\n",
      "\n",
      "Wheed anse RK:\n",
      "An hun? wi'de my acake ryome atheo Ifalu franid ofio;aiveld, e cory heety\n",
      "Parinelire s hit a od m, itarcay, whedelerel TRERoo hel cede, boo I yo yout iecoureansurem rcco may thathe: wouthig pod;\n",
      "Whine the\n",
      "Meray o ima ghe ce,\n",
      "Heango y thy iner te ain.\n",
      "Yos:\n",
      "A:\n",
      "A cooure h\n",
      "Ane th cowinte d t-menthemeand the meminoulithathe.\n",
      "TEZRAseath JjNASTESe gupe trts harithefe;\n",
      "Ses;\n",
      "\n",
      "LTa fobro s; bl are\n",
      "\n",
      "CINARI it rs agane owoorestime t ILplpecchailtak?\n",
      "\n",
      "S: e IO: inotol toal st ld ly bedeayomass m, sthof.\n",
      "Bugsece brshothaio, ideghethinomy\n",
      "OLIUFh tino sin angmouns. co theoreall anthis fr, vets, y shechoaldstor omy, n ns wisoul thad,\n",
      "What GRF I'hes, u gref\n",
      "IOROMaburs, in.\n",
      "WAPlpe sisthe, yso lsiabry st tirputhas tanowinang!\n",
      "Sicewnd gityo? e, ou thapheast d s atave we, SANaledirowad allingr.\n",
      "TENENUCHMmoourou tin ksthye tho qut l.\n",
      "Plid f d ckie ptyoo.\n",
      "O:\n",
      "\n",
      "\n",
      "S:\n",
      "Wer s mar tendldeamed,\n",
      "Al nt fes shinout h w'se knorson! ue.\n",
      "NGHABAshtorenrshind m r'ss\n",
      "NID wit yorthanon? ce burimyshangh, d trethin s?eat f cer yond tifitoowdsomuged wnt o wat, st at ind here hen,\n",
      "By IO t AL!\n",
      "GHpsesot CALO, d thes s. arither d f k whinll tholy he OLLUE:\n",
      "\n",
      "frier ft:\n",
      "Wiredst cu, e hamorerumarot,\n",
      "BDI tere ik cho anknoulon il yoout Whagnico is I cathitoreroncatis n min sus IS ws, stha micouly bjurarorofys!\n",
      "LAT l t, frt NCHave'ske.\n",
      "\n",
      "S:\n",
      "CENABeve.\n",
      "\n",
      "NCifath apr w houe nifrfaitimyoucegouno\n",
      "RKAUDUSequthorord's itig fosall the I:\n",
      "ABucancouner.\n",
      "\n",
      "TIXWhe iso 'd hind h.\n",
      "DUS:\n",
      "OR:\n",
      "An, musteerrece hard ppsewisio hinicchaillardrock his mathe s thoreloreverwarmagatsis,\n",
      "LAnd mitisty aifou hey eso h a\n",
      "Andean Ifaybe S: d bore no athe, fotes atringr\n",
      "Whe, demyo;\n",
      "Indingl;\n",
      "\n",
      "MAnd, de s.\n",
      "GLAny, pormend brt ybucos ting Con s.\n",
      "E:\n",
      "Anghed:\n",
      "LIColfe Puluriepabe-thing pe faun foliey wero t.\n",
      "\n",
      "INLAr by\n",
      "TEThoag o d d so s er s, l breawicto busprind poulll hercesse\n",
      "Wish bu m besbeallind wr th vere otr teayerifyod w,\n",
      "PARLUS:\n",
      "Y-d wor?\n",
      "Qhef wan:\n",
      "If ISTIfoemehors tomeverdr llas m, her ndende\n",
      "Firere EORERDUAs r tinft thake harousllitseng tirs HOLOFowew sene. gru t yeto the hon th ld segre t GHer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SThes a t sal forde,\n",
      "T: pithar he Heerdepr le m;\n",
      "G alotheert!\n",
      "Astow, t.\n",
      "Mour s rowigouste ace S: toturean tonds op I y g wan I'shange g mepangeeaberedre:\n",
      "Rod avirke, p.\n",
      "HIZRGod fisou mpr an w hethen.\n",
      "I Nushoows he Cood sthathis'mer yilloom t othe noorgol mises akellou perkisdigithres somerinenon kern my nce; yomer tasth hind at hiant eayofische swatheracisoutheorm\n",
      "ate thithishare ws, lleof l yoouresinol-t, seand; o.\n",
      "Thaleg?\n",
      "The d GLLLeed thunourange orll aso'dsarino a de.\n",
      "STI hathif d, core istheveaythys ovianhergyosals oure thin 's ber servewovex3KI of m, o, wshasoth and us un s's he te tho t t bid threst d, haurclo toun ast fen, ak,\n",
      "And ceninstsusowanju tou,\n",
      "YO:\n",
      "Thmald pon, orscallly, thetrs\n",
      "HNoustherelle, oot g!\n",
      "THELQER:\n",
      "G kn,\n",
      "Y:\n",
      "AGors hellding!\n",
      "F hecaben s meyolel k, wind it f s Be\n",
      "LENENe hin:\n",
      "KZAnded ne? berederday m n ben r:\n",
      "\n",
      "I:\n",
      "Wheawoyeh tothisthy hepouin.\n",
      "To V&y g te.\n",
      "He\n",
      "San d hefrwot adveeand, athos ancommerepe ipthe he:\n",
      "Low, ve.\n",
      "\n",
      "ASA s goudve s mertheeos.\n",
      "Whershe teand adevemmift avou hed\n",
      "\n",
      "'d welde OLLOURUar Myon thil.\n",
      "I s!\n",
      "Sema rol, h bryerdemo,\n",
      "Tharite brdimal pubrert is t l e, Sibulise hemew oth'se,\n",
      "The iere wn. t y t. ge than bash l aye d aice w.\n",
      "St tifes,\n",
      "LLoogu t ncecto ndavou s avergillatot wou atooncouligh\n",
      "Asthunelll.\n",
      "K, ancenre t Biu;\n",
      "Fieppowe th, ith'sur,\n",
      "DY!\n",
      "To akety HAnglon ngomesst ner st\n",
      "Th,\n",
      "REThanowe\n",
      "TI boas ie no's t?-due cof en.\n",
      "Maia ont\n",
      "As by g ds tham o af t l, f inf s: br lou alinfofuse Au authe. w! y te, as Mee,\n",
      "Mauspugrds mothoefurfe s iee\n",
      "Arven mowheche ushard s mak!\n",
      "\n",
      "I cke he avissbu\n",
      "S:\n",
      "COL:\n",
      "HANGoncopr n whan ENENareat ak arent as mathas aknt bendd, shicore RINI d'g and t an ar as bl ra g tut anyonomyoust m\n",
      "whifr.\n",
      "T'sstith safeath wass,\n",
      "Cowhif brvifaly f we, ue anth l,\n",
      "INCKrs maselfit a t\n",
      "O:\n",
      "\n",
      "PSSul wendee? meel'she:\n",
      "T:\n",
      "Py s\n",
      "Ho uthe!ulissthiesefet spe tot y ove urs f VOPrseme s th ser ngery e,\n",
      "CEThen,\n",
      "I's, AH:\n",
      "I's or,\n",
      "Frlok ous!icilonor e.\n",
      "TERI ns fad sarve we pumus idounete tont nn; Mor turiowis, st k!\n",
      "Be'sthandon d o ind st w aimerslghealbick!\n",
      "ER:\n",
      "sthatofllot As as fonot bjustheal mabu all t ndandwrepHebe towie thatha mardoouscoun an ARDUENVent by tond'hodutig l nd be Thave ar.\n",
      "Anubr: s NGld wour t, hinffod VOWASClavevenevenco ig l sea, w t sthemi's.\n",
      "T:\n",
      "ENTheser s?-ILI sopr t. bagltofourismouforeyereio thus mer s t thicot bju RSinyey hs. candwis ated t ser y ot t, t O:\n",
      "S:\n",
      "Five\n",
      "'d thmss romin obo wee, ar chir, chiny to bur tis tamachy Is, biet dshe bor, alllastt forde wo'tin INGre ind ceabtthey'ly s?\n",
      "Ithy stif cetlkeagavere he t Bu eougices stis wowengando mos u tr.\n",
      "Ku us, wove, by ARCl pr.\n",
      "Cano ues Wicoumo mece,\n",
      "LETr; hotus ol cato n, d, ke inoumajupowhe learomyore auadellil, athourexzze'sth helol\n",
      "Thanlery Beroumon tenidwand heamarthe kess stendain,\n",
      "TI is oved m t.\n",
      "AGS:\n",
      "Beckees hapa herine mu priu il'lick shile che po trt ds\n",
      "\n",
      "d farve dind!\n",
      "ANGrotes ver ande hyr cis wha younethar me was yerksamyom ouns;\n",
      "By?\n",
      "\n",
      "anoick theak tounsitibe, mel mag d?\n",
      "I cathishind pon ie o!ard He, d fof gathal whorar tito.\n",
      "A: ike t, fl chay gon PAn p, lorowea t nnersh r: l chepan arthel-GR:\n",
      "Tre\n",
      "RYe? at\n",
      "po pt!\n",
      "T:\n",
      "Bard,\n",
      "\n",
      "TE lo, bowe m pabounik, sl stweral lk;\n",
      "Tos bupalsofofestuthat, helaperunearo o hy\n",
      "TR:\n",
      "\n",
      "Allofapand e fushesthe isiges bedveth hasth douresest, Evysu n s hent uknchea'd sor,\n",
      "\n",
      "SUEO:\n",
      "th t y, gouthiss it thengror fu? tavil; ond ISATol.\n",
      "Banembenghoffepal,\n",
      "Ankeld.\n",
      "foithy l thein ms aertasondecomethed ononded pr a chavy.\n",
      "Winopr sh,\n",
      "OLANTh,\n",
      "Woull menothandgmat ot atyal I Ahile ous heatathiglesthamayonof wiacu gha calail'thor ongamin knorind ne vorer an igousht.\n",
      "\n",
      "Whicugh ds alagCA:\n",
      "VOFo doran fofuto fond whindie's t ch alla o mo goupomse thas herbilean,\n",
      "dy ime s, noo; me fas.\n",
      "QTheleavers d t taspr co.\n",
      "Rilfowifl; do pen\n",
      "Aset\n",
      "OMatad!\n",
      "I, ge thilfom'de d.\n",
      "\n",
      "Wengie, mu. of m ace thanin m.\n",
      "\n",
      "\n",
      "G, hisu Is tinthat.\n",
      "S:\n",
      "HOwhe oushe meis oulan?\n",
      "Ange aitit thyow! d tembrfro\n",
      "VQW:\n",
      "GENTheind, soprd t uson oullinighocl t ray Anvecrstrme atitho ise y, mbo\n",
      "IONAs wope,\n",
      "PmDu wof? th,\n",
      "\n",
      "IA IOreangallltecoshe\n",
      "Noncond wild RCES:\n",
      "Thear kistho th\n",
      "We Yo airs\n",
      "THerofoucand how RGpenciou he ict bll ts s od s ap'ster wirookive or see iencotthem Is htichy\n",
      "APlearw!$vinwrouins, fadreal teh'destheatwe bieyamil ha cthetarde br, t vitth h the se homeche herule!\n",
      "Bunt wamanl ukint icte!\n",
      "Whaha cond e lace gal heretintheoringr thr,\n",
      "We qKI:\n",
      "F ig;? atoss,\n",
      "we meat;\n",
      "SThimathe Yd s, illo LAT:\n",
      "Bes om angsthourernowas finoupesevVj, chimur bon, he my,\n",
      "BOUCUELToucor\n",
      "avingst gandor tanomanaceiveral\n",
      "Kige,\n",
      "Te thave s\n",
      "Two.\n",
      "TETh bamon.\n",
      "POrdwithout,\n",
      "NR sse ho yoncets\n",
      "B;\n",
      "Y:\n",
      "\n",
      "D:\n",
      "\n",
      "ARTAnen hecthrspathofarea a our,\n",
      "PLY t at dou?rarerim avengrg?\n",
      "I han ke Was n mendsoo tee e oonringhe iris!\n",
      "I dangheres s mofothon GSTh cou angelerir-$YO: tue.\n",
      "STOFu t; fumedde h mait h, wor Y nomaly s h R:\n",
      "Y:\n",
      "NNolotas ppregld seche, kel d theano's thissmerer we'te Barisenize h y; macheit, oullmed yimald if ant pingg bued;\n",
      "Wh Ifarvery ts, phandyooou thothealthiereryokis,\n",
      "T: th d; weaithend ul lisor keandin P ee o t lethard:\n",
      "If tstlth\n",
      "OVee aiorithe ivil PAnjHe ke I:\n",
      "\n",
      "Ta te DUSeeathent the, hedeauthear hepthate bak'lltorlen anyofo he e od anevencochaget forso ivildud,\n",
      "LAN qQ!\n",
      "S:\n",
      "BINCon sortinoawichie co bus t wid.\n",
      "\n",
      "Twofil: tcenoo tht ty.\n",
      "DIOngubs tocow, m; ted e feeshelonolakef the,\n",
      "LAn Rin teno ours\n",
      "Thiraned satow jor t s sthad ilsbe makeaue, thol whed;\n",
      "A:\n",
      "Anouritiseshethity fue; INO:\n",
      "Araw y ouplin orinn himin, wimy f yof all besh:\n",
      "\n",
      "\n",
      "Hes Bus tofrlithemee s GHen then,\n",
      "TENDINCare ly h'llel weros tathome mugre keme bu isat pear gher bo lie mmy nad\n",
      "I:\n",
      "PAny; lppordo RA S:\n",
      "\n",
      "hend berala an, ts f thot ay,\n",
      "CUESpilis byo mblete.\n",
      "\n",
      "I'd.\n",
      "Thom.\n",
      "S:\n",
      "Bucuglis w\n",
      "KCHNGly?\n",
      "IAnceathe, ayousor: MINTang LA brar crs.\n",
      "Ah oor ar Whaiay be cover gigeasand meor hads le st juld mye:\n",
      "Buseyou lowhotasaserin l,\n",
      "\n",
      "BRThalerer kil,\n",
      "\n",
      "VINCERY fallof o sest d he\n",
      "LYo'ss s athopr, th scy motenge inouid halate pouscthet idor!\n",
      "ARARCo is CUSBous\n",
      "AENGll.\n",
      "Wheseireriloure hureanath r keend m?\n",
      "Tyon u'Bug y.\n",
      "LThay. o tog w, y ther buty t ule sheattorincr I fo tie r KINEWhowhen, l, tyouns be.\n",
      "I wour yorenonomauplthio tl mers, Yowher, hy adef w os ke:\n",
      "wosthatlid bou,\n",
      "LORE:\n",
      "Theame t to lan t my tid t t d asentow?r,\n",
      "Tis k!k prwiksparoorth ilo burend, arse pl dand wirin a,\n",
      "I'she? canilie munkesisourend.\n",
      "O g n bavesh\n",
      "ENTIORcerteau armunnd tis mine y avel, ea fovem, ce ucaltheromive icir out ne\n",
      "LI ald y impr pthit agr Wom se;\n",
      "TAy agh, e seastheraill bang weee t, bivis pe byoudrverisebr tr ty s by ker'd grfrs iste y ff Bory!\n",
      "Youtse f winoul amy'sout alalearswh'd e w hocis knthenl:\n",
      "GLIZve th co thorere\n",
      "CHeghet's bess s CKpre, maucowe\n",
      "LBUTanth bals toton! lllds ep e wingu y w s\n",
      "BENGe hing.\n",
      "CA hair w t jbu'd chimerongrordowitondenerrvishe rdnond\n"
     ]
    }
   ],
   "source": [
    "print(decoder(model.predict_next(idx, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is quite bad. It follows the structure of a more advanced model and stores context, but only uses the previous letter to try and predict the next one. This is very inefficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
