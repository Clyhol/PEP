{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is an example of how to make tokens communicate with previous tokens. It is important that they do not communicate with future tokens.\n",
    "\n",
    "The easiest way for making them communitate is to average the channels of the previous tokens. This created a feature vector which summarizes the previous tokens. This is called self-attention.\n",
    "\n",
    "Averageing the channels is very lossy, as it doesn't take sequences or time into account.\n",
    "\n",
    "The point here is that we want to iterate batch-wise over the tokens and average the channels of the previous tokens.\n",
    "\n",
    "\n",
    "B is the batch size. This means how many subsets of data are we running in parallel\n",
    "\n",
    "T is the time steps. This determines how many tokens should be in each batch\n",
    "\n",
    "C is the number of features in each time step. This is the resolution for our encoding. Setting this to 2 would mean that each token is represented as a 2D feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# torch.manual_seed(1337)  # seeded randomness\n",
    "# B, T, C = 4, 8, 2  # batch size, time steps, number of classes\n",
    "# x = torch.randn(B, T, C)  # random input\n",
    "# print(x.shape)\n",
    "\n",
    "# xbow = torch.zeros((B, T, C))\n",
    "# for b in range(B):\n",
    "#     for t in range(T):\n",
    "#         xprev = x[b, :t + 1]\n",
    "#         xbow[b, t] = torch.mean(xprev, dim=0)\n",
    "# print(x[0], \"\\n\")\n",
    "# print(xbow[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done more effeciently using matricies by making a lower triangular matrix, A, and multiplying it to B. By normalizing the A matrix this will end up as an average.\n",
    "\n",
    "![alt text](https://algebra1course.wordpress.com/wp-content/uploads/2013/02/slide10.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n",
      "----------------\n",
      "tensor([[-0.5832,  0.2666,  0.2533,  0.2091,  1.0909],\n",
      "        [ 0.4585,  1.6043, -0.3486,  1.6032, -0.3804],\n",
      "        [ 1.6613, -1.2887, -0.3822, -0.1208, -0.7130],\n",
      "        [ 0.5640, -0.0485, -0.7641,  0.9784,  1.2115],\n",
      "        [-0.4730, -0.3698,  0.3643, -0.1631, -1.2849]])\n",
      "----------------\n",
      "tensor([[-5.8317e-01,  2.6657e-01,  2.5327e-01,  2.0913e-01,  1.0909e+00],\n",
      "        [-6.2317e-02,  9.3542e-01, -4.7654e-02,  9.0618e-01,  3.5528e-01],\n",
      "        [ 5.1222e-01,  1.9403e-01, -1.5916e-01,  5.6386e-01, -8.3263e-04],\n",
      "        [ 5.2516e-01,  1.3341e-01, -3.1040e-01,  6.6749e-01,  3.0225e-01],\n",
      "        [ 3.2552e-01,  3.2767e-02, -1.7546e-01,  5.0137e-01, -1.5171e-02]])\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "a = torch.tril(torch.ones((5,5)))\n",
    "a = a / a.sum(dim=1, keepdim=True)\n",
    "b = torch.randn(5, 5)\n",
    "c = a @ b # dot product\n",
    "\n",
    "print(a)\n",
    "print(\"----------------\")\n",
    "print(b)\n",
    "print(\"----------------\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Average normalization\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# previous a is the attention weights\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# previous b is the (B,T,C) tensor\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtril(torch\u001b[38;5;241m.\u001b[39mones((\u001b[43mT\u001b[49m,T))) \u001b[38;5;66;03m# size equal to number of tokens in a sequence\u001b[39;00m\n\u001b[0;32m      7\u001b[0m weights \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;241m/\u001b[39m weights\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# normalize the weights\u001b[39;00m\n\u001b[0;32m      8\u001b[0m xbow \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;241m@\u001b[39m x \u001b[38;5;66;03m# (B, T, T) @ (B, T, C) = (B, T, C)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "# Average normalization\n",
    "\n",
    "# previous a is the attention weights\n",
    "# previous b is the (B,T,C) tensor\n",
    "\n",
    "weights = torch.tril(torch.ones((T,T))) # size equal to number of tokens in a sequence\n",
    "weights = weights / weights.sum(dim=-1, keepdim=True) # normalize the weights\n",
    "xbow = weights @ x # (B, T, T) @ (B, T, C) = (B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way of implementing this, is using softmax as the normalization. Here we set all 0's in the lower triangular matrix to '-inf', which gives us the same result for the weight matrix when we apply softmax.\n",
    "\n",
    "This method allows tokens to decide which other tokens from the past they want to communicate with. This is explained as affinity in the lecure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones((T,T)))\n",
    "weights = torch.zeros((T,T)) # initialize weights at 0\n",
    "weights = weights.masked_fill(tril == 0, float(\"-inf\"))\n",
    "weights = F.softmax(weights, dim=-1) # softmax over the time dimension (x-axis)\n",
    "xbow = weights @ x # (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally Self-attention implementation\n",
    "From Andrej Karpathy video\n",
    "\n",
    "Self-attention solves making the current token chose what other tokens to listen to more or less (control the affinity), by making a Query and a Key\n",
    "\n",
    "The key holds information about what a token contains\n",
    "The query is information about what the token wants.\n",
    "A high alignment between these leads to a high value when taking the dot product. This is self-attention.\n",
    "The difference here is that the weights are no longer a constant adding up to 1, but instead data driven weights, normalized to sum to 1.\n",
    "This can be seen especially on the last row of the weights matrix. This is the 8th token, meaning it has as much context as possible for a single batch. In the matrix the values represent the affinity strength, meaning how much the token on that position matches the query of the current token.\n",
    "\n",
    "For this example it will be implemented for a single head.\n",
    "\n",
    "#### Notes from video\n",
    "Attention is a communication mechanism that acts on nodes in a directed manner. They can follow any structure you want, but need defined rules of what node can talk to which. In the case of NLP it is often linearly scaling like below. Attention is also not reliant on space, like convolutions are. Attention is simply a bunch of vectors with private information, that communicate how well their key aligns with others querys. Space aspects can and will be added in the model, through a positional embedding table.\n",
    "\n",
    "Another important note is that batches are completely separate. Although they are scooped in the same data loader, each batch should be seen individually, running in parallel.\n",
    "For *sentiment analysis* it may be okay for all nodes to talk to each other, as it is no longer about predicting the future. This is implemented by simpy removing the mask.\n",
    "\n",
    "A head with a mask is called a *decoder block*\n",
    "A head without a mask is called an *encoder block*\n",
    "![image.png](self-attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5713e-01,  8.8009e-01,  1.6152e-01, -7.8239e-01, -1.4289e-01,\n",
       "          7.4676e-01,  1.0068e-01, -5.2395e-01, -8.8726e-01,  1.9068e-01,\n",
       "          1.7616e-01, -5.9426e-01, -4.8124e-01, -4.8598e-01,  2.8623e-01,\n",
       "          5.7099e-01],\n",
       "        [ 4.3974e-01, -1.4227e-01, -1.3157e-01,  2.8895e-03, -1.3222e-01,\n",
       "          6.6082e-04, -2.7904e-01, -2.2676e-01, -2.8723e-01,  5.7456e-01,\n",
       "          5.6053e-01, -2.5208e-01,  9.7243e-02,  1.0771e-01,  3.0455e-02,\n",
       "          1.0727e+00],\n",
       "        [ 4.3615e-01, -6.6358e-02, -2.9296e-01,  7.4315e-02,  5.4381e-02,\n",
       "         -7.0388e-02, -6.8984e-02, -8.2153e-02, -2.9377e-01, -5.8952e-02,\n",
       "          3.5887e-01, -2.3087e-03, -1.8212e-01, -3.6142e-02, -6.7189e-02,\n",
       "          1.1412e+00],\n",
       "        [ 4.2069e-01, -1.0619e-01, -2.9984e-01,  5.2820e-02,  2.0077e-01,\n",
       "         -1.6048e-01, -3.5710e-02, -8.3110e-02, -1.7919e-01,  7.7992e-02,\n",
       "          1.2719e-01,  2.2611e-02, -5.1811e-02,  7.4466e-02,  1.8131e-01,\n",
       "          8.4463e-01],\n",
       "        [ 3.9499e-01,  1.7130e-01,  5.1664e-02,  2.0128e-01,  2.4059e-01,\n",
       "          1.6471e-01,  1.9638e-01,  1.3151e-01, -3.0257e-01, -3.9997e-01,\n",
       "         -4.7060e-02, -6.8541e-02, -3.7259e-01,  1.4653e-01,  3.3643e-02,\n",
       "          7.8407e-01],\n",
       "        [ 3.2160e-01,  1.3167e-01,  3.4681e-02,  2.6722e-01,  2.1268e-01,\n",
       "          1.6392e-01,  1.1234e-01,  7.3362e-02, -2.4218e-01, -2.6597e-01,\n",
       "          2.2721e-02, -1.5014e-02, -2.8530e-01,  1.6292e-01,  7.6938e-02,\n",
       "          7.5743e-01],\n",
       "        [ 1.0560e-01,  4.5449e-02, -1.3713e-01,  2.3461e-01,  1.8927e-01,\n",
       "         -2.0829e-02, -4.4675e-02, -6.8756e-02, -1.2469e-01,  4.6523e-02,\n",
       "          1.0449e-01,  9.9329e-02, -1.0045e-02,  7.7849e-02,  1.9440e-01,\n",
       "          6.4730e-01],\n",
       "        [ 1.2431e-01,  4.5290e-02, -3.4119e-01,  2.7087e-01,  2.3352e-01,\n",
       "         -9.4792e-02, -4.2095e-02,  2.1426e-01, -3.2988e-02, -3.1300e-02,\n",
       "          5.1987e-02,  2.3780e-01,  1.0845e-01, -9.5935e-02,  2.9991e-02,\n",
       "          4.7065e-01]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)  # seeded randomness\n",
    "B,T,C = 4,8,32 # batch size, time steps, number of classes\n",
    "x = torch.randn(B,T,C) # random input\n",
    "\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias = False) # linear transformation of token features\n",
    "query = nn.Linear(C, head_size, bias=False) # linear transformation of token wants to attend to\n",
    "value = nn.Linear(C, head_size, bias=False) # linear transformation of token features\n",
    "k = key(x) # (B, T, C) -> (B, T, head_size)\n",
    "q = query(x) # (B, T, C) -> (B, T, head_size)\n",
    "v = value(x) # (B, T, C) -> (B, T, head_size)\n",
    "weights = q @ k.transpose(1,2) * head_size ** -0.5# ((B, T, head_size) @ (B, head_size, T))/sqrt(head_size) = (B, T, T)\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones((T,T)))\n",
    "\n",
    "#TODO try removing this mask for sentiment analysis\n",
    "weights = weights.masked_fill(tril == 0, float(\"-inf\")) # mask out the upper triangular part. This is to prevent the model from attending to future tokens\n",
    "weights = F.softmax(weights, dim=-1) # softmax over the time dimension (x-axis)\n",
    "out = weights @ v # (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "\n",
    "out[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiheaded attention\n",
    "Multihead attention is simply creating more attention blocks, and concatenating their answers. Typically you want to keep the same dimensionality, so you divide the head_size with the amount of heads that are being used. This ends up being concatenated to the same size, but the benefit is that each head can be initialized independently and can run in parallel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
