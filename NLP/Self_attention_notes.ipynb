{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is an example of how to make tokens communicate with previous tokens. It is important that they do not communicate with future tokens.\n",
    "\n",
    "The easiest way for making them communitate is to average the channels of the previous tokens. This created a feature vector which summarizes the previous tokens. This is called self-attention.\n",
    "\n",
    "Averageing the channels is very lossy, as it doesn't take sequences or time into account.\n",
    "\n",
    "The point here is that we want to iterate batch-wise over the tokens and average the channels of the previous tokens.\n",
    "\n",
    "\n",
    "B is the batch size. This means how many subsets of data are we running in parallel\n",
    "\n",
    "T is the time steps. This determines how many tokens should be in each batch\n",
    "\n",
    "C is the number of features in each time step. This is the resolution for our encoding. Setting this to 2 would mean that each token is represented as a 2D feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]]) \n",
      "\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)  # seeded randomness\n",
    "B, T, C = 4, 8, 2  # batch size, time steps, number of classes\n",
    "x = torch.randn(B, T, C)  # random input\n",
    "print(x.shape)\n",
    "\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t + 1]\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)\n",
    "print(x[0], \"\\n\")\n",
    "print(xbow[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done more effeciently using matricies by making a lower triangular matrix, A, and multiplying it to B. By normalizing the A matrix this will end up as an average.\n",
    "\n",
    "![alt text](https://algebra1course.wordpress.com/wp-content/uploads/2013/02/slide10.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n",
      "----------------\n",
      "tensor([[-0.8345,  0.5978, -0.0514, -0.0646, -0.4970],\n",
      "        [ 0.4658, -0.2573, -1.0673,  2.0089, -0.9665],\n",
      "        [ 0.3583,  0.1073,  1.2463,  1.2460,  0.3534],\n",
      "        [ 0.9425, -1.6669, -0.7960,  0.1298, -1.9446],\n",
      "        [ 0.0610, -0.2379,  1.9020, -1.1763, -0.1772]])\n",
      "----------------\n",
      "tensor([[-0.8345,  0.5978, -0.0514, -0.0646, -0.4970],\n",
      "        [-0.1844,  0.1703, -0.5593,  0.9722, -0.7318],\n",
      "        [-0.0035,  0.1493,  0.0426,  1.0635, -0.3700],\n",
      "        [ 0.2330, -0.3048, -0.1671,  0.8300, -0.7637],\n",
      "        [ 0.1986, -0.2914,  0.2467,  0.4288, -0.6464]])\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "a = torch.tril(torch.ones((5,5)))\n",
    "a = a / a.sum(dim=1, keepdim=True)\n",
    "b = torch.randn(5, 5)\n",
    "c = a @ b # dot product\n",
    "\n",
    "print(a)\n",
    "print(\"----------------\")\n",
    "print(b)\n",
    "print(\"----------------\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average normalization\n",
    "\n",
    "# previous a is the attention weights\n",
    "# previous b is the (B,T,C) tensor\n",
    "\n",
    "weights = torch.tril(torch.ones((T,T))) # size equal to number of tokens in a sequence\n",
    "weights = weights / weights.sum(dim=1, keepdim=True) # normalize the weights\n",
    "xbow = weights @ x # (B, T, T) @ (B, T, C) = (B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way of implementing this, is using softmax as the normalization. Here we set all 0's in the lower triangular matrix to '-inf', which gives us the same result for the weight matrix when we apply softmax.\n",
    "\n",
    "This method allows tokens to decide which other tokens from the past they want to communicate with. This is explained as affinity in the lecure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones((T,T)))\n",
    "weights = torch.zeros((T,T)) # initialize weights at 0\n",
    "weights = weights.masked_fill(tril == 0, float(\"-inf\"))\n",
    "weights = F.softmax(weights, dim=1) # softmax over the time dimension (x-axis)\n",
    "xbow = weights @ x # (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "xbow[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
